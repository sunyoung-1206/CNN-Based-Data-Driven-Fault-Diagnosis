{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94be2131-2eb0-44c7-bd2f-31724730b2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\admin\\\\박선영\\\\CNN-Based-Data-Driven-Fault-Diagnosis'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n",
    "#%pip install glob\n",
    "#%pip install numpy\n",
    "#%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c11b115-8dde-46ce-9021-9dcd5f5ee94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow[gpu] in c:\\users\\admin\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorflow[gpu]) (0.5.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow[gpu]) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow[gpu]) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow[gpu]) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\admin\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow[gpu]) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[gpu]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[gpu]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[gpu]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow[gpu]) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow[gpu]) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow[gpu]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow[gpu]) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow[gpu]) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow[gpu]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow[gpu]) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow[gpu]) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "2.19.0\n",
      "TensorFlow GPU 사용 가능 여부: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: tensorflow 2.19.0 does not provide the extra 'gpu'\n"
     ]
    }
   ],
   "source": [
    "#%pip install tensorflow[gpu]\n",
    "#import tensorflow as tf\n",
    "#print(tf.__version__)\n",
    "\n",
    "#print(\"TensorFlow GPU 사용 가능 여부:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b51fae46-44cc-4409-b961-56dc6028b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3;C:\\Users\\admin\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\admin\\anaconda3\\Library\\usr\\bin;C:\\Users\\admin\\anaconda3\\Library\\bin;C:\\Users\\admin\\anaconda3\\Scripts;C:\\Users\\admin\\anaconda3\\bin;C:\\Users\\admin\\anaconda3\\condabin;C:\\Users\\admin\\anaconda3;C:\\Users\\admin\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\admin\\anaconda3\\Library\\usr\\bin;C:\\Users\\admin\\anaconda3\\Library\\bin;C:\\Users\\admin\\anaconda3\\Scripts;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0;C:\\WINDOWS\\System32\\OpenSSH;C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;C:\\Program Files\\Git\\cmd;C:\\Program Files\\PuTTY;C:\\Program Files\\dotnet;C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR;C:\\Users\\admin\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\admin\\AppData\\Local\\Programs\\Microsoft VS Code\\bin;C:\\Program Files\\JetBrains\\PyCharm 2024.1.4\\bin;.;.\n"
     ]
    }
   ],
   "source": [
    "#echo %PATH%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781e457f-5c1f-463a-8d16-77574501dd97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc39e6-85ca-417b-a924-c6150edfa80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6343c3d2-8845-485d-8d09-ae8b9503ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 폴더 생성 성공: ./generated_data(2000)\\train_images, ./generated_data(2000)\\test_images\n",
      "📁 발견된 CSV 파일 수: 40\n",
      "📊 파일: BF_0_0.007.csv, 데이터 포인트 수: 122571, ✅ 충분함\n",
      "📊 파일: BF_0_0.014.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: BF_0_0.021.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: BF_1_0.007.csv, 데이터 포인트 수: 121410, ✅ 충분함\n",
      "📊 파일: BF_1_0.014.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: BF_1_0.021.csv, 데이터 포인트 수: 121701, ✅ 충분함\n",
      "📊 파일: BF_2_0.007.csv, 데이터 포인트 수: 121556, ✅ 충분함\n",
      "📊 파일: BF_2_0.014.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: BF_2_0.021.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: BF_3_0.007.csv, 데이터 포인트 수: 121556, ✅ 충분함\n",
      "📊 파일: BF_3_0.014.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: BF_3_0.021.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: IF_0_0.007.csv, 데이터 포인트 수: 121265, ✅ 충분함\n",
      "📊 파일: IF_0_0.014.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: IF_0_0.021.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: IF_1_0.007.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: IF_1_0.014.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: IF_1_0.021.csv, 데이터 포인트 수: 121556, ✅ 충분함\n",
      "📊 파일: IF_2_0.007.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: IF_2_0.014.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: IF_2_0.021.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: IF_3_0.007.csv, 데이터 포인트 수: 122917, ✅ 충분함\n",
      "📊 파일: IF_3_0.014.csv, 데이터 포인트 수: 121701, ✅ 충분함\n",
      "📊 파일: IF_3_0.021.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: OF_0_0.007.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: OF_0_0.014.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: OF_0_0.021.csv, 데이터 포인트 수: 122426, ✅ 충분함\n",
      "📊 파일: OF_1_0.007.csv, 데이터 포인트 수: 122426, ✅ 충분함\n",
      "📊 파일: OF_1_0.014.csv, 데이터 포인트 수: 122136, ✅ 충분함\n",
      "📊 파일: OF_1_0.021.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: OF_2_0.007.csv, 데이터 포인트 수: 121410, ✅ 충분함\n",
      "📊 파일: OF_2_0.014.csv, 데이터 포인트 수: 121846, ✅ 충분함\n",
      "📊 파일: OF_2_0.021.csv, 데이터 포인트 수: 122281, ✅ 충분함\n",
      "📊 파일: OF_3_0.007.csv, 데이터 포인트 수: 122571, ✅ 충분함\n",
      "📊 파일: OF_3_0.014.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: OF_3_0.021.csv, 데이터 포인트 수: 121991, ✅ 충분함\n",
      "📊 파일: normal_0.csv, 데이터 포인트 수: 243938, ✅ 충분함\n",
      "📊 파일: normal_1.csv, 데이터 포인트 수: 483903, ✅ 충분함\n",
      "📊 파일: normal_2.csv, 데이터 포인트 수: 483903, ✅ 충분함\n",
      "📊 파일: normal_3.csv, 데이터 포인트 수: 485643, ✅ 충분함\n",
      "\n",
      "🔄 처리 중: BF_0_0.007.csv (라벨 0)\n",
      "📈 가능한 최대 샘플 수: 118476\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_0_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_0_0.014.csv (라벨 1)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_0_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_0_0.021.csv (라벨 2)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_0_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_1_0.007.csv (라벨 0)\n",
      "📈 가능한 최대 샘플 수: 117315\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_1_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_1_0.014.csv (라벨 1)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_1_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_1_0.021.csv (라벨 2)\n",
      "📈 가능한 최대 샘플 수: 117606\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_1_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_2_0.007.csv (라벨 0)\n",
      "📈 가능한 최대 샘플 수: 117461\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_2_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_2_0.014.csv (라벨 1)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_2_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_2_0.021.csv (라벨 2)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_2_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_3_0.007.csv (라벨 0)\n",
      "📈 가능한 최대 샘플 수: 117461\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_3_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_3_0.014.csv (라벨 1)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_3_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: BF_3_0.021.csv (라벨 2)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ BF_3_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_0_0.007.csv (라벨 6)\n",
      "📈 가능한 최대 샘플 수: 117170\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_0_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_0_0.014.csv (라벨 7)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_0_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_0_0.021.csv (라벨 8)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_0_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_1_0.007.csv (라벨 6)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_1_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_1_0.014.csv (라벨 7)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_1_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_1_0.021.csv (라벨 8)\n",
      "📈 가능한 최대 샘플 수: 117461\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_1_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_2_0.007.csv (라벨 6)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_2_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_2_0.014.csv (라벨 7)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_2_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_2_0.021.csv (라벨 8)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_2_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_3_0.007.csv (라벨 6)\n",
      "📈 가능한 최대 샘플 수: 118822\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_3_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_3_0.014.csv (라벨 7)\n",
      "📈 가능한 최대 샘플 수: 117606\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_3_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: IF_3_0.021.csv (라벨 8)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ IF_3_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_0_0.007.csv (라벨 3)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_0_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_0_0.014.csv (라벨 4)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_0_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_0_0.021.csv (라벨 5)\n",
      "📈 가능한 최대 샘플 수: 118331\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_0_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_1_0.007.csv (라벨 3)\n",
      "📈 가능한 최대 샘플 수: 118331\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_1_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_1_0.014.csv (라벨 4)\n",
      "📈 가능한 최대 샘플 수: 118041\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_1_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_1_0.021.csv (라벨 5)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_1_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_2_0.007.csv (라벨 3)\n",
      "📈 가능한 최대 샘플 수: 117315\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_2_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_2_0.014.csv (라벨 4)\n",
      "📈 가능한 최대 샘플 수: 117751\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_2_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_2_0.021.csv (라벨 5)\n",
      "📈 가능한 최대 샘플 수: 118186\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_2_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_3_0.007.csv (라벨 3)\n",
      "📈 가능한 최대 샘플 수: 118476\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_3_0.007.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_3_0.014.csv (라벨 4)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_3_0.014.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: OF_3_0.021.csv (라벨 5)\n",
      "📈 가능한 최대 샘플 수: 117896\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ OF_3_0.021.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: normal_0.csv (라벨 9)\n",
      "📈 가능한 최대 샘플 수: 239843\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ normal_0.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: normal_1.csv (라벨 9)\n",
      "📈 가능한 최대 샘플 수: 479808\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ normal_1.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: normal_2.csv (라벨 9)\n",
      "📈 가능한 최대 샘플 수: 479808\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ normal_2.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "🔄 처리 중: normal_3.csv (라벨 9)\n",
      "📈 가능한 최대 샘플 수: 481548\n",
      "📋 생성 예정: 학습 데이터 2000개, 테스트 데이터 400개\n",
      "✅ normal_3.csv 처리 완료: 학습 데이터 2000개, 테스트 데이터 400개 생성됨\n",
      "\n",
      "📑 처리된 파일 수: 40/40\n",
      "✅ 엑셀 파일 저장 성공: ./generated_data(2000)\\trainingImageList.xlsx (80000개 항목)\n",
      "✅ 엑셀 파일 저장 성공: ./generated_data(2000)\\validationImageList.xlsx (16000개 항목)\n",
      "\n",
      "📊 파일별 처리 결과 요약:\n",
      "==================================================\n",
      "파일명                            학습 데이터       테스트 데이터     \n",
      "--------------------------------------------------\n",
      "BF_0_0.007.csv                 2000         400         \n",
      "BF_0_0.014.csv                 2000         400         \n",
      "BF_0_0.021.csv                 2000         400         \n",
      "BF_1_0.007.csv                 2000         400         \n",
      "BF_1_0.014.csv                 2000         400         \n",
      "BF_1_0.021.csv                 2000         400         \n",
      "BF_2_0.007.csv                 2000         400         \n",
      "BF_2_0.014.csv                 2000         400         \n",
      "BF_2_0.021.csv                 2000         400         \n",
      "BF_3_0.007.csv                 2000         400         \n",
      "BF_3_0.014.csv                 2000         400         \n",
      "BF_3_0.021.csv                 2000         400         \n",
      "IF_0_0.007.csv                 2000         400         \n",
      "IF_0_0.014.csv                 2000         400         \n",
      "IF_0_0.021.csv                 2000         400         \n",
      "IF_1_0.007.csv                 2000         400         \n",
      "IF_1_0.014.csv                 2000         400         \n",
      "IF_1_0.021.csv                 2000         400         \n",
      "IF_2_0.007.csv                 2000         400         \n",
      "IF_2_0.014.csv                 2000         400         \n",
      "IF_2_0.021.csv                 2000         400         \n",
      "IF_3_0.007.csv                 2000         400         \n",
      "IF_3_0.014.csv                 2000         400         \n",
      "IF_3_0.021.csv                 2000         400         \n",
      "OF_0_0.007.csv                 2000         400         \n",
      "OF_0_0.014.csv                 2000         400         \n",
      "OF_0_0.021.csv                 2000         400         \n",
      "OF_1_0.007.csv                 2000         400         \n",
      "OF_1_0.014.csv                 2000         400         \n",
      "OF_1_0.021.csv                 2000         400         \n",
      "OF_2_0.007.csv                 2000         400         \n",
      "OF_2_0.014.csv                 2000         400         \n",
      "OF_2_0.021.csv                 2000         400         \n",
      "OF_3_0.007.csv                 2000         400         \n",
      "OF_3_0.014.csv                 2000         400         \n",
      "OF_3_0.021.csv                 2000         400         \n",
      "normal_0.csv                   2000         400         \n",
      "normal_1.csv                   2000         400         \n",
      "normal_2.csv                   2000         400         \n",
      "normal_3.csv                   2000         400         \n",
      "==================================================\n",
      "\n",
      "✅ 데이터셋 생성 완료!\n",
      "📊 총 생성된 데이터: 학습 데이터 80000개 / 테스트 데이터 16000개\n",
      "📂 학습 이미지 폴더: ./generated_data(2000)\\train_images\n",
      "📂 테스트 이미지 폴더: ./generated_data(2000)\\test_images\n",
      "📄 학습 데이터 목록: ./generated_data(2000)\\trainingImageList.xlsx\n",
      "📄 테스트 데이터 목록: ./generated_data(2000)\\validationImageList.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from PIL import Image\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "# 1. 폴더 및 저장 경로 설정\n",
    "DATA_DIR = \"./data\"  # 원본 데이터 파일 위치\n",
    "OUTPUT_DIR = \"./generated_data(2000)\"\n",
    "TRAIN_IMAGE_DIR = os.path.join(OUTPUT_DIR, \"train_images\")\n",
    "TEST_IMAGE_DIR = os.path.join(OUTPUT_DIR, \"test_images\")\n",
    "\n",
    "# 폴더 생성 (권한 확인을 위한 예외 처리 추가)\n",
    "try:\n",
    "    os.makedirs(TRAIN_IMAGE_DIR, exist_ok=True)\n",
    "    os.makedirs(TEST_IMAGE_DIR, exist_ok=True)\n",
    "    print(f\"✅ 폴더 생성 성공: {TRAIN_IMAGE_DIR}, {TEST_IMAGE_DIR}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 폴더 생성 실패: {str(e)}\")\n",
    "\n",
    "# 2. 라벨 매핑 (BF, OF, IF, 정상 상태)\n",
    "label_mapping = {\n",
    "    \"BF_0_0.007\": 0, \"BF_0_0.014\": 1, \"BF_0_0.021\": 2,\n",
    "    \"BF_1_0.007\": 0, \"BF_1_0.014\": 1, \"BF_1_0.021\": 2,\n",
    "    \"BF_2_0.007\": 0, \"BF_2_0.014\": 1, \"BF_2_0.021\": 2,\n",
    "    \"BF_3_0.007\": 0, \"BF_3_0.014\": 1, \"BF_3_0.021\": 2,\n",
    "    \"OF_0_0.007\": 3, \"OF_0_0.014\": 4, \"OF_0_0.021\": 5,\n",
    "    \"OF_1_0.007\": 3, \"OF_1_0.014\": 4, \"OF_1_0.021\": 5,\n",
    "    \"OF_2_0.007\": 3, \"OF_2_0.014\": 4, \"OF_2_0.021\": 5,\n",
    "    \"OF_3_0.007\": 3, \"OF_3_0.014\": 4, \"OF_3_0.021\": 5,\n",
    "    \"IF_0_0.007\": 6, \"IF_0_0.014\": 7, \"IF_0_0.021\": 8,\n",
    "    \"IF_1_0.007\": 6, \"IF_1_0.014\": 7, \"IF_1_0.021\": 8,\n",
    "    \"IF_2_0.007\": 6, \"IF_2_0.014\": 7, \"IF_2_0.021\": 8,\n",
    "    \"IF_3_0.007\": 6, \"IF_3_0.014\": 7, \"IF_3_0.021\": 8,\n",
    "    \"normal_0\": 9, \"normal_1\": 9, \"normal_2\": 9, \"normal_3\": 9\n",
    "}\n",
    "\n",
    "# 3. 파일 로드 및 디버깅 정보 추가\n",
    "csv_files = sorted(glob(os.path.join(DATA_DIR, \"*.csv\")))\n",
    "print(f\"📁 발견된 CSV 파일 수: {len(csv_files)}\")\n",
    "if len(csv_files) == 0:\n",
    "    print(f\"❌ 파일을 찾을 수 없습니다. 경로를 확인하세요: {os.path.join(DATA_DIR, '*.csv')}\")\n",
    "    print(f\"  현재 작업 디렉토리: {os.getcwd()}\")\n",
    "\n",
    "# 라벨 매핑 디버깅\n",
    "unmapped_files = []\n",
    "for file_path in csv_files:\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "    if file_name not in label_mapping:\n",
    "        unmapped_files.append(file_name)\n",
    "if unmapped_files:\n",
    "    print(f\"⚠ 라벨 매핑에 없는 파일들: {unmapped_files}\")\n",
    "\n",
    "# 데이터 포인트 개수 확인\n",
    "file_data_points = {}\n",
    "for file_path in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        data = df.apply(pd.to_numeric, errors='coerce').to_numpy().ravel()\n",
    "        data = data[~np.isnan(data)]\n",
    "        file_name = os.path.basename(file_path)\n",
    "        file_data_points[file_name] = len(data)\n",
    "        print(f\"📊 파일: {file_name}, 데이터 포인트 수: {len(data)}\" + \n",
    "              (\", ✅ 충분함\" if len(data) >= 4096 else f\", ❌ 부족함 (최소 4096 필요)\"))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 읽기 오류: {file_path}, {str(e)}\")\n",
    "\n",
    "# 학습/테스트 데이터 저장용 리스트\n",
    "train_data_list = []\n",
    "test_data_list = []\n",
    "\n",
    "# 실제 생성된 데이터 개수를 추적\n",
    "valid_train_count = 0\n",
    "valid_test_count = 0\n",
    "\n",
    "# 각 파일별 생성 데이터 추적용 딕셔너리\n",
    "file_train_counts = {}\n",
    "file_test_counts = {}\n",
    "\n",
    "# 4. 데이터 처리 및 이미지 변환 (4096개 선택 후 전처리) - 디버깅 정보 추가\n",
    "def process_and_save_images(file_path, label, train_count=2000, test_count=400):\n",
    "    global valid_train_count, valid_test_count\n",
    "    \n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\n🔄 처리 중: {file_name} (라벨 {label})\")\n",
    "    \n",
    "    \"\"\" 주어진 파일에서 데이터를 읽어 이미지로 변환하고 저장 \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None)  # CSV 파일 로드\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')  # 문자열이 포함된 경우 숫자로 변환\n",
    "        data = df.to_numpy().ravel()  # 1D numpy 배열 변환\n",
    "        data = data[~np.isnan(data)]  # NaN 값 제거\n",
    "        total_data_points = len(data)\n",
    "        \n",
    "        # 데이터가 부족하면 카운트하지 않고 종료\n",
    "        if total_data_points < 4096:\n",
    "            print(f\"❌ 데이터 부족: {file_path} (데이터 개수: {total_data_points}), 최소 4096개 필요 - 스킵됨\")\n",
    "            return\n",
    "            \n",
    "        # 가능한 샘플 수 계산\n",
    "        max_possible_samples = total_data_points - 4096 + 1\n",
    "        \n",
    "        # 실제 생성할 샘플 수 결정\n",
    "        actual_train_count = min(train_count, max_possible_samples)\n",
    "        actual_test_count = min(test_count, max_possible_samples)\n",
    "        \n",
    "        print(f\"📈 가능한 최대 샘플 수: {max_possible_samples}\")\n",
    "        print(f\"📋 생성 예정: 학습 데이터 {actual_train_count}개, 테스트 데이터 {actual_test_count}개\")\n",
    "        \n",
    "        # 학습용 데이터 생성\n",
    "        train_indices = random.sample(range(0, max_possible_samples), actual_train_count)\n",
    "        train_success_count = 0\n",
    "        \n",
    "        for i in train_indices:\n",
    "            segment = data[i:i + 4096]  # 4096개 데이터 선택\n",
    "            \n",
    "            # 최소값과 최대값 확인 (전처리 디버깅)\n",
    "            min_val = np.min(segment)\n",
    "            max_val = np.max(segment)\n",
    "            \n",
    "            # 최소값과 최대값이The assistant can create and refernce artifacts during conversations. Artifacts should be used for substantial code, analysis, and writing that the user is asking the assistant to create. the same인 경우 전처리 불가\n",
    "            if min_val == max_val:\n",
    "                print(f\"⚠ 전처리 불가: 데이터 세그먼트의 최소값과 최대값이 같음 ({min_val})\")\n",
    "                continue\n",
    "                \n",
    "            # **전처리 공식 적용**\n",
    "            segment = np.round((segment - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "            \n",
    "            # 64x64로 변환\n",
    "            segment = segment.reshape(64, 64)\n",
    "            img_path = os.path.join(TRAIN_IMAGE_DIR, f\"train_{label}_{valid_train_count}.png\")\n",
    "            \n",
    "            # 파일 저장 및 확인\n",
    "            try:\n",
    "                Image.fromarray(segment).convert(\"L\").save(img_path)\n",
    "                train_data_list.append([f\"train_{label}_{valid_train_count}.png\", label])\n",
    "                valid_train_count += 1  # 유효한 학습 데이터 증가\n",
    "                train_success_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 이미지 저장 오류 (학습 데이터): {img_path}, {str(e)}\")\n",
    "                continue  # 저장 실패 시 카운트하지 않음\n",
    "        \n",
    "        # 테스트용 데이터 생성\n",
    "        test_indices = random.sample(range(0, max_possible_samples), actual_test_count)\n",
    "        test_success_count = 0\n",
    "        \n",
    "        for i in test_indices:\n",
    "            segment = data[i:i + 4096]  # 4096개 데이터 선택\n",
    "            \n",
    "            # 최소값과 최대값 확인 (전처리 디버깅)\n",
    "            min_val = np.min(segment)\n",
    "            max_val = np.max(segment)\n",
    "            \n",
    "            # 최소값과 최대값이 같은 경우 전처리 불가\n",
    "            if min_val == max_val:\n",
    "                print(f\"⚠ 전처리 불가: 데이터 세그먼트의 최소값과 최대값이 같음 ({min_val})\")\n",
    "                continue\n",
    "                \n",
    "            # **전처리 공식 적용**\n",
    "            segment = np.round((segment - min_val) / (max_val - min_val) * 255).astype(np.uint8)\n",
    "            \n",
    "            # 64x64로 변환\n",
    "            segment = segment.reshape(64, 64)\n",
    "            img_path = os.path.join(TEST_IMAGE_DIR, f\"test_{label}_{valid_test_count}.png\")\n",
    "            \n",
    "            # 파일 저장 및 확인\n",
    "            try:\n",
    "                Image.fromarray(segment).convert(\"L\").save(img_path)\n",
    "                test_data_list.append([f\"test_{label}_{valid_test_count}.png\", label])\n",
    "                valid_test_count += 1  # 유효한 테스트 데이터 증가\n",
    "                test_success_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ 이미지 저장 오류 (테스트 데이터): {img_path}, {str(e)}\")\n",
    "                continue  # 저장 실패 시 카운트하지 않음\n",
    "        \n",
    "        # 현재 파일 처리 결과 기록\n",
    "        file_train_counts[file_name] = train_success_count\n",
    "        file_test_counts[file_name] = test_success_count\n",
    "        \n",
    "        print(f\"✅ {file_name} 처리 완료: 학습 데이터 {train_success_count}개, 테스트 데이터 {test_success_count}개 생성됨\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 처리 중 오류 발생: {file_path}, {str(e)}\")\n",
    "\n",
    "# 5. 모든 파일 처리\n",
    "processed_file_count = 0\n",
    "for file_path in csv_files:\n",
    "    file_name = os.path.basename(file_path).replace(\".csv\", \"\")\n",
    "    # 라벨 매핑\n",
    "    if file_name in label_mapping:\n",
    "        label = label_mapping[file_name]\n",
    "        process_and_save_images(file_path, label)\n",
    "        processed_file_count += 1\n",
    "    else:\n",
    "        print(f\"⚠ 라벨 매핑 없음 (건너뜀): {file_name}\")\n",
    "\n",
    "print(f\"\\n📑 처리된 파일 수: {processed_file_count}/{len(csv_files)}\")\n",
    "\n",
    "# 6. 엑셀 파일 생성 (헤더 없이 저장)\n",
    "def save_to_excel(data_list, file_name):\n",
    "    \"\"\" 데이터 리스트를 엑셀 파일로 저장 (헤더 제거) \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(data_list)\n",
    "        output_path = os.path.join(OUTPUT_DIR, file_name)\n",
    "        df.to_excel(output_path, index=False, header=False)  # 헤더 제거\n",
    "        print(f\"✅ 엑셀 파일 저장 성공: {output_path} ({len(data_list)}개 항목)\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 엑셀 파일 저장 실패: {file_name}, {str(e)}\")\n",
    "\n",
    "save_to_excel(train_data_list, \"trainingImageList.xlsx\")\n",
    "save_to_excel(test_data_list, \"validationImageList.xlsx\")\n",
    "\n",
    "# 7. 각 파일별 처리 결과 요약\n",
    "print(\"\\n📊 파일별 처리 결과 요약:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'파일명':<30} {'학습 데이터':<12} {'테스트 데이터':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for file_name in sorted(set(list(file_train_counts.keys()) + list(file_test_counts.keys()))):\n",
    "    train_count = file_train_counts.get(file_name, 0)\n",
    "    test_count = file_test_counts.get(file_name, 0)\n",
    "    print(f\"{file_name:<30} {train_count:<12} {test_count:<12}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 8. 총 처리 결과\n",
    "print(f\"\\n✅ 데이터셋 생성 완료!\")\n",
    "print(f\"📊 총 생성된 데이터: 학습 데이터 {valid_train_count}개 / 테스트 데이터 {valid_test_count}개\")\n",
    "print(f\"📂 학습 이미지 폴더: {TRAIN_IMAGE_DIR}\")\n",
    "print(f\"📂 테스트 이미지 폴더: {TEST_IMAGE_DIR}\")\n",
    "print(f\"📄 학습 데이터 목록: {os.path.join(OUTPUT_DIR, 'trainingImageList.xlsx')}\")\n",
    "print(f\"📄 테스트 데이터 목록: {os.path.join(OUTPUT_DIR, 'validationImageList.xlsx')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc8234-a13b-42c2-b96e-fc44b7a69f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
